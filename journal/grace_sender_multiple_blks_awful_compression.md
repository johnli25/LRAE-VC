# Why Small Blocks Inflate Compressed Size

When a 32,768-element `torch.float16` tensor is split into 32 blocks of 1,024 elements and each block is pickled and compressed individually, the total compressed size (\~300 KB) becomes far larger than compressing the tensor as one block (\~10 KB). This happens due to a combination of pickle overhead on each small object and reduced compression efficiency on many small chunks. Below, we detail the reasons and compare the outcomes, then discuss how to avoid this problem.

## Pickle Overhead for Each Chunk

Serializing each small tensor block separately adds a significant constant overhead per block. Python’s pickle includes metadata (object type, shape, data type, etc.) with each serialized object. For numeric arrays, the pickle output is essentially similar to NumPy’s `.npy` format, which carries a header with information like the array’s shape and dtype. This header is on the order of a couple hundred bytes per array. Key points regarding pickle overhead are:

* **Repeated Metadata:** Each of the 32 blocks, when pickled, contains its own copy of metadata such as the tensor’s dtype (`float16`) and shape (1,024). This overhead (roughly \~100–200 bytes per array) is duplicated 32 times, inflating the total size. In contrast, pickling the entire tensor as one object includes a single header for the whole dataset.
* **Float16 Consideration:** For a `float16` tensor, the data buffer is half the size of a float32 buffer for the same number of elements, meaning the fixed overhead is relatively **larger** compared to each block’s data. (For example, \~200 bytes of header on only 2 KB of data per block is \~10% overhead before compression.) This effect is “especially” pronounced for float16 because the payload per block is smaller, making the pickle overhead a bigger fraction of each block’s data.
* **Python Object Serialization:** If the tensor chunks were converted to Python objects (e.g. lists of floats) before pickling, the overhead would be even worse. Pickle would then serialize each float as an individual Python object, adding opcodes and possibly converting half-precision floats to 64-bit representations. This can dramatically bloat the serialized size of each chunk. (In our case, we assume the chunks remain as NumPy/PyTorch arrays, so the overhead is mostly the array header.)

In short, splitting the tensor forces pickle to repeat the same framing information 32 times. That alone contributes on the order of a few kilobytes of redundant data in the compressed output. But the much larger factor is how compression behaves on small vs. large inputs, as explained next.

## Inefficiency of zlib on Small Blocks

Compression algorithms like zlib’s deflate work best on large data streams where they can find and exploit redundancy. When you compress many small blocks independently, each block is treated as a separate stream, which is **inherently less efficient**:

* **Fresh Dictionary for Each Block:** Deflate is a dictionary-based compressor (LZ77). Each time you start compressing a new block, the compressor begins with an empty history. Small inputs give the algorithm “very little time to learn and adapt from input”, meaning it can’t build up a good dictionary of repeated patterns within that tiny chunk. As a result, “the smaller the amount to compress, the worse the compression ratio” – there just isn’t enough data in one 1 KB–2 KB block for the compressor to find many redundancies before the block ends.
* **Compression Headers and Huffman Tables:** Each independent zlib stream has its own headers and termination markers. By default, zlib adds a small header (\~2 bytes) and checksum footer (4 bytes) per compressed stream, plus it may include a Huffman coding table for the block. Even if the data in a small block is totally random (incompressible), zlib will output it either as a “stored” (uncompressed) block or with a Huffman encoding, but **still with a few bytes of overhead**. In fact, deflate always adds a 5-byte overhead per block (even for uncompressed blocks up to 16 KB) plus a one-time 6-byte stream overhead. For very tiny inputs, this overhead can dominate – e.g. compressing a 1-byte input yields \~11 bytes output (1100% expansion) due to zlib’s header/footer. In our case of 2 KB blocks, each compressed chunk carries around 5–11 bytes of zlib overhead, which multiplied by 32 adds a few hundred bytes extra.
* **Poor Compression in Isolation:** If the tensor data had any repetitive patterns or zero regions, a large single-block compression would capture those across the entire 64 KB. But within each 2 KB chunk, those patterns might be shorter or cut off. A small chunk might not contain enough repetitions to justify a efficient Huffman coding. Often for very small inputs, the deflate algorithm can’t achieve any size reduction at all. It might even fallback to storing the data uncompressed (plus overhead), resulting in little to no size decrease for each chunk. Essentially, small independent blocks “offer relatively poor prospects for compression” – many chunks will compress only slightly or even expand.

Because of these factors, zlib is much less effective when applied to 32 separate 2 KB streams than to one 64 KB stream. The compressor cannot reuse patterns found in earlier blocks, and it pays the cost of stream headers and re-learning Huffman codes on every block.

## One Large Buffer vs. Many Small Buffers

When compressing the entire tensor as a single block, the compression algorithm sees the full 64 KB of data at once. All redundancy in the tensor (even if it spans across what would have been chunk boundaries) can be exploited in one pass. The dictionary that deflate builds can reference repeats anywhere in the 32K-window of data, and the Huffman coding is optimized for the global frequency of byte patterns in the whole tensor.

In contrast, splitting into 32 streams loses those advantages:

* **Lost Cross-Block Redundancy:** Any pattern that spans from the end of one chunk into the beginning of the next is invisible to the compressor when chunks are separate. For example, if a sequence of bytes repeats every 4 KB, the combined compression could replace long runs of that sequence with back-references. But chunking at 2 KB boundaries means each chunk sees at most half of the sequence, not enough to detect the repetition. As a result, the compressed chunks still carry what a single combined compression would have eliminated.
* **Reset of Compression Context:** With one large input, the compression context (history dictionary and statistical model) is continuous. With many small inputs, the context resets for each chunk. This inherently *degrades the compression ratio* – as one expert notes, “splitting data into blocks will only degrade compression ratio” (the benefit of splitting is only for random access or parallelism, not for size). In our scenario, the single-block compression achieved a \~6.5:1 compression ratio, but when forced to reset every 2 KB, the effective ratio per chunk was much worse.
* **Repeated Structural Overhead:** Not only does each pickle have repeated metadata, but from the compressor’s perspective, each chunk may contain similar structure bytes (from the pickle format) that appear in every chunk’s input. When compressing as one large buffer, if the same header or sequence (e.g., the pickle opcodes for a float16 array) appears 32 times, the deflate algorithm will notice this repetition and greatly shorten its representation in the output. Separate compression can’t do this – each chunk’s compressor sees the header bytes **once** and has to emit them literally in the compressed data. Thus, what would have been a single copy of the metadata in the combined output becomes 32 copies across the separate outputs.

**Size comparison:** The net effect is dramatic. Compressing the entire 32K×float16 tensor as one object yielded only \~**10 KB** of compressed data, whereas compressing 32 small blocks yielded around **300 KB** in total. In other words, the piecemeal approach produced an output about 30× larger. Each 2 KB block on average blew up to roughly 9 KB compressed (far larger than the block’s raw size!), whereas as one unit the 64 KB tensor shrank to 10 KB. This discrepancy is explained by the repeated \~0.2 KB pickle overhead per block and the loss of global compression efficiency – the large single block compression was able to pack the data much tighter by leveraging patterns across the whole tensor and amortizing headers.

## Improving Packetization Without Blowing Up Size

To avoid this massive overhead, it’s best to **compress once, then split** for transport:

* **Compress the Whole Tensor First:** Serialize the entire tensor (e.g. via `pickle.dumps` or, even more efficiently, get a raw bytes buffer of the tensor) and apply `zlib.compress` **one time** to the full data. This produces a single compressed byte stream (\~10 KB in our example).
* **Slice the Compressed Stream into Chunks:** You can then cut the compressed byte stream into 32 smaller byte chunks (for example, for network packetization). Since these are just pieces of one continuous compressed stream, they will be around a few hundred bytes each (instead of 9 KB each). Importantly, they are **not independently decompressible** – they must be reassembled in order – but as long as the receiver concatenates them in the correct order, the final byte stream will decompress to the original tensor.
* **Reassemble and Decompress on the Receiving Side:** Once all chunks are received, concatenate them and run a single `zlib.decompress` (or use a streaming decompression) to get back the full tensor object. This way, you pay the pickle and compression overhead only once, not on every fragment.

By compressing upfront, you preserve the efficiency of a large compression dictionary and avoid repeated headers. Essentially, you retain the \~10 KB total size, while still sending in 32 smaller packets. This approach addresses network MTU or message size limits without sacrificing compression ratio.

Other alternatives and considerations include:

* **Use Streaming Compression:** Instead of calling `zlib.compress` separately for each piece, use the zlib streaming interface (`zlib.compressobj`) to compress the data in parts. This allows you to emit data in chunks for transmission while maintaining a continuous compression context internally. The result is similar to compressing once then slicing, but it gives you the ability to flush out bytes as you go. (This is more complex, but effectively achieves the same goal: one compression dictionary spanning all data.)
* **Send Raw Data With Less Overhead:** If feasible, minimize serialization overhead before compression. For example, you could send the raw binary tensor data and its shape/dtype metadata separately. A 64 KB raw buffer of float16 values can often be compressed more directly than a 70 KB pickle payload that contains additional opcodes. In practice, PyTorch’s pickling is reasonably efficient for tensors, but in some cases using a format like NumPy `.npy` or a custom header + raw bytes can shave off a few hundred bytes that don’t need to be compressed at all.
* **Consider Compression Alternatives:** If you truly needed each chunk to be individually compressed and decompressed (independent packets), standard deflate will always struggle to compress such small blocks. In those cases, you might look into **dictionary compression** techniques or other algorithms. For instance, zlib and zstd support providing a preset dictionary to use for multiple small messages so that each small block starts with some prior knowledge. This is advanced and only worth it if you must decode chunks out of order or independently. In most scenarios, it’s simpler to stick with a single compression over the whole data, as described above.

**Conclusion:** Splitting a float16 tensor into many pieces causes a blow-up in compressed size because each piece incurs \~constant pickle overhead and cannot benefit from cross-chunk redundancy during compression. Compressing the data as one large buffer yields a far superior compression ratio, thanks to one-time metadata and a global compression dictionary. The recommended solution is to compress the tensor once (thus taking full advantage of zlib’s dictionary-based compression over the entire data) and then slice the compressed output for transmission. This preserves the \~10 KB size instead of ballooning to hundreds of KB, while still allowing you to send in smaller packets. By using these strategies, you avoid zlib’s worst-case overhead on small blocks and achieve efficient network packetization without the payload bloat.

**Sources:**

* Python pickle overhead for NumPy arrays (includes \~200 byte header with shape & dtype)
* Explanation of poor compression on smaller data blocks
* Zlib deflate format overhead for small blocks (5 bytes per 16KB block + 6 bytes per stream)
* Guidance that splitting data into small blocks hurts compression ratio
