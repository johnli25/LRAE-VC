{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from transformers import FlavaProcessor, FlavaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNC_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNC_Autoencoder, self).__init__()\n",
    "        self.encoder1 = nn.Conv2d(3, 16, kernel_size=9, stride=7, padding=4)\n",
    "        self.encoder2 = nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.decoder1 = nn.ConvTranspose2d(10, 64, kernel_size=9, stride=7, padding=4, output_padding=6)\n",
    "        self.decoder2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.decoder3 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.final_layer = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, tail_length=None):\n",
    "        x1 = self.relu(self.encoder1(x))\n",
    "        x2 = self.relu(self.encoder2(x1))\n",
    "        if tail_length is not None:\n",
    "            batch_size, channels, _, _ = x2.size()\n",
    "            tail_start = channels - tail_length\n",
    "            x2 = x2.clone()\n",
    "            x2[:, tail_start:, :, :] = 0\n",
    "        y1 = self.relu(self.decoder1(x2))\n",
    "        y2 = self.relu(self.decoder2(y1)) + y1\n",
    "        y3 = self.relu(self.decoder3(y2))\n",
    "        y4 = self.relu(self.decoder3(y3)) + y3\n",
    "        y5 = self.final_layer(y4)\n",
    "        return torch.clamp(y5, min=0, max=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNC_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNC_Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder1 = nn.Conv2d(3, 16, kernel_size=9, stride=7, padding=4)\n",
    "        self.encoder2 = nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder1 = nn.ConvTranspose2d(10, 64, kernel_size=9, stride=7, padding=4, output_padding=6)\n",
    "        self.decoder2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.decoder3 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.final_layer = nn.Conv2d(64, 3, kernel_size=3, stride=1,  padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, tail_length=None):\n",
    "        # Encoder\n",
    "        x1 = self.relu(self.encoder1(x))\n",
    "        x2 = self.relu(self.encoder2(x1))\n",
    "\n",
    "        if tail_length is not None:\n",
    "            # Zero out tail features for all samples in the batch\n",
    "            batch_size, channels, _, _ = x2.size()\n",
    "            tail_start = channels - tail_length\n",
    "            x2 = x2.clone() # Create a copy of the tensor to avoid in-place operations!\n",
    "            x2[:, tail_start:, :, :] = 0\n",
    "\n",
    "        # Decoder\n",
    "        y1 = self.relu(self.decoder1(x2))\n",
    "        y2 = self.relu(self.decoder2(y1))\n",
    "        y2 = y2 + y1  # Skip connection\n",
    "        y3 = self.relu(self.decoder3(y2))\n",
    "        y4 = self.relu(self.decoder3(y3))\n",
    "        y4 = y4 + y3  # Skip connection\n",
    "        y5 = self.final_layer(y4)\n",
    "        y5 = torch.clamp(y5, min=0, max=1)\n",
    "\n",
    "        return y5\n",
    "    \n",
    "\n",
    "\n",
    "class LRAE_VC_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LRAE_VC_Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3),  # (3, 224, 224) -> (32, 112, 112)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),  # (32, 112, 112) -> (64, 56, 56)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # (64, 56, 56) -> (128, 28, 28)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # (128, 28, 28) -> (64, 56, 56)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # (64, 56, 56) -> (32, 112, 112)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # (32, 112, 112) -> (16, 224, 224)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.final_layer = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1)  # (16, 224, 224) -> (3, 224, 224)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder1(x)  # (32, 112, 112)\n",
    "        x2 = self.encoder2(x1)  # (64, 56, 56)\n",
    "        x3 = self.encoder3(x2)  # (128, 28, 28)\n",
    "\n",
    "        # Decoder\n",
    "        y1 = self.decoder1(x3)  # (64, 56, 56)\n",
    "        y1 = y1 + x2  # Skip connection\n",
    "\n",
    "        y2 = self.decoder2(y1)  # (32, 112, 112)\n",
    "        y2 = y2 + x1  # Skip connection\n",
    "\n",
    "        y3 = self.decoder3(y2)  # (16, 224, 224)\n",
    "\n",
    "        y4 = self.final_layer(self.dropout(y3))  # (3, 224, 224)\n",
    "        y5 = self.sigmoid(y4)  # Normalize output to [0, 1]\n",
    "        return y5\n",
    "\n",
    "\n",
    "\n",
    "class PNC_Autoencoder_with_Classification(nn.Module):\n",
    "    def __init__(self, num_classes=10, classes=['diving', 'golf_front', 'kick_front', 'lifting', 'riding_horse', 'running', 'skating', 'swing_bench', 'swing_side', 'walk_front']): # Default classes derived from UCF-101\n",
    "        super(PNC_Autoencoder_with_Classification, self).__init__()\n",
    "\n",
    "        self.classes = classes\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = nn.Conv2d(3, 16, kernel_size=9, stride=7, padding=4)  # (3, 224, 224) -> (16, 32, 32)\n",
    "        self.encoder2 = nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1)  # (16, 32, 32) -> (10, 32, 32)\n",
    "\n",
    "        # Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(10 * 32 * 32, 256),  # Flattened bottleneck size -> 256 hidden units\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)  # Final layer for classification\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder1 = nn.ConvTranspose2d(10, 64, kernel_size=9, stride=7, padding=4, output_padding=6)  # (10, 32, 32) -> (64, 224, 224)\n",
    "        self.decoder2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)  # (64, 224, 224) -> (64, 224, 224)\n",
    "        self.decoder3 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)  # (64, 224, 224) -> (64, 224, 224)\n",
    "        self.final_layer = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)  # (64, 224, 224) -> (3, 224, 224)\n",
    "\n",
    "        # Activation Functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  # For output normalization in range [0, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.relu(self.encoder1(x))  # (16, 32, 32)\n",
    "        x2 = self.relu(self.encoder2(x1))  # (10, 32, 32)\n",
    "\n",
    "        # Decoder\n",
    "        y1 = self.relu(self.decoder1(x2))  # (64, 224, 224)\n",
    "        y2 = self.relu(self.decoder2(y1))  # (64, 224, 224)\n",
    "        y2 = y2 + y1 # Skip connection\n",
    "        y3 = self.relu(self.decoder3(y2))  # (64, 224, 224)\n",
    "        y4 = self.relu(self.decoder3(y3))  # (64, 224, 224)\n",
    "        y4 = y4 + y3  # Skip connection\n",
    "        y5 = self.final_layer(y4)  # (3, 224, 224)\n",
    "        y5 = torch.clamp(y5, min=0, max=1)  # Ensure output is in [0, 1] range\n",
    "\n",
    "        # Classification\n",
    "        class_scores = self.classifier(x2)  # (10, 32, 32) -> (num_classes)\n",
    "\n",
    "        return y5, class_scores # Return (decoded image, class output label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.img_names = os.listdir(img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        gt_path = os.path.join(self.gt_dir, self.img_names[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        ground_truth = Image.open(gt_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            ground_truth = self.transform(ground_truth)\n",
    "        return image, ground_truth, self.img_names[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and training\n",
    "def train_autoencoder(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, model_name, max_tail_length):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets, _ in train_loader:\n",
    "            # Sample a single tail length for the batch\n",
    "            torch.manual_seed(seed=42)\n",
    "            tail_len = torch.randint(0, max_tail_length + 1, (1,)).item()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, tail_length=tail_len) # NOTE: set tail_length=None to use the full sequence OR set tail_length=tail_length to enable stochastic tail-dropout\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = test_autoencoder(model, val_loader, criterion, device, max_tail_length)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"{model_name}_best_validation.pth\")\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]: Validation loss improved. Model saved.\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Final Test\n",
    "    test_loss = test_autoencoder(model, test_loader, criterion, device, max_tail_length)\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    plot_train_val_loss(train_losses, val_losses)\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), f\"{model_name}_final.pth\")\n",
    "\n",
    "\n",
    "def test_autoencoder(model, dataloader, criterion, device, max_tail_length):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, _ in dataloader:\n",
    "            torch.manual_seed(seed=41)\n",
    "            tail_len = torch.randint(0, max_tail_length + 1, (1,)).item()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs, tail_length=tail_len) # NOTE: set tail_length=None to use the full sequence OR set tail_length=tail_length to enable stochastic tail-dropout\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return test_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_combined_model(model, train_loader, val_loader, test_loader, criterion_reconstruction, criterion_classification, optimizer, device, num_epochs, model_name):\n",
    "    print(f\"Training {model_name} with combined loss...\")\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets, _ in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, class_scores = model(inputs)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            loss_reconstruction = criterion_reconstruction(outputs, targets)\n",
    "\n",
    "            # Classification loss\n",
    "            labels = get_labels_from_filename(_)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "            loss_classification = criterion_classification(class_scores, labels)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = loss_reconstruction + loss_classification\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = test_combined_model(model, val_loader, criterion_reconstruction, criterion_classification, device)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"{model_name}_best_validation.pth\")\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]: Validation loss improved. Model saved.\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Final Testing\n",
    "    test_loss = test_combined_model(model, test_loader, criterion_reconstruction, criterion_classification, device)\n",
    "    print(f\"Final Test Loss for {model_name}: {test_loss:.4f}\")\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), f\"{model_name}_final.pth\")\n",
    "\n",
    "def test_combined_model(model, dataloader, criterion_reconstruction, criterion_classification, device, final_test=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_predictions = []\n",
    "    filenames_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, img_file_name in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs, class_scores = model(inputs)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            loss_reconstruction = criterion_reconstruction(outputs, targets)\n",
    "\n",
    "            # Classification loss\n",
    "            labels = get_labels_from_filename(img_file_name)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "            loss_classification = criterion_classification(class_scores, labels)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = loss_reconstruction + loss_classification\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Predict class labels\n",
    "            _, predicted_labels = torch.max(class_scores, 1) # shape is (batch_size,) since it's batch_size of predictions\n",
    "            all_predictions.extend(predicted_labels.cpu().numpy()) # ... that's why use .extend() because it's a list of predictions\n",
    "            filenames_list.extend(img_file_name)\n",
    "\n",
    "    if final_test:\n",
    "        # Print predictions\n",
    "        for filename, prediction in zip(filenames_list, all_predictions):\n",
    "            print(f\"Image: {filename}, Predicted Label: {prediction}\")\n",
    "\n",
    "    return test_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "def get_labels_from_filename(filenames, class_map = {\n",
    "        \"diving\": 0, \"golf_front\": 1, \"kick_front\": 2, \"lifting\": 3, \"riding_horse\": 4,\n",
    "        \"running\": 5, \"skating\": 6, \"swing_bench\": 7, \"swing_side\": 8, \"walk_front\": 9\n",
    "    }):\n",
    "    labels = []\n",
    "    for filename in filenames:\n",
    "        activity = \"_\".join(filename.split(\"_\")[:-2])\n",
    "        labels.append(class_map[activity])\n",
    "    return labels\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Train the PNC Autoencoder or PNC Autoencoder with Classification.\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, choices=[\"PNC\", \"PNC_with_classification\", \"LRAE_VC\"], \n",
    "                        help=\"Model to train\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def plot_train_val_loss(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss', color='blue', marker='o')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "    \n",
    "    plt.title('Training and Validation Loss', fontsize=16)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('train_val_loss_curve.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "img_height, img_width = 224, 224  # Dependent on autoencoder architecture\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "path = \"UCF_224x224x3_PNC_FrameCorr_input_imgs/\"\n",
    "\n",
    "dataset = ImageDataset(path, path, transform=transform)\n",
    "\n",
    "# Define test dataset using specified filenames\n",
    "test_img_names = [\n",
    "    \"diving_7\", \"diving_8\", \"golf_front_7\", \"golf_front_8\", \"kick_front_8\", \"kick_front_9\",\n",
    "    \"lifting_5\", \"lifting_6\", \"riding_horse_8\", \"riding_horse_9\", \"running_7\", \"running_8\",\n",
    "    \"running_9\", \"skating_8\", \"skating_9\", \"swing_bench_7\", \"swing_bench_8\", \"swing_bench_9\"\n",
    "]\n",
    "\n",
    "test_indices = [\n",
    "    i for i in range(len(dataset))\n",
    "    if \"_\".join(dataset.img_names[i].split(\"_\")[:-1]) in test_img_names\n",
    "]\n",
    "train_val_indices = [i for i in range(len(dataset)) if i not in test_indices]\n",
    "\n",
    "# Split train_val_indices into train and validation\n",
    "np.random.shuffle(train_val_indices)\n",
    "train_size = int(0.8 * len(train_val_indices))\n",
    "val_size = len(train_val_indices) - train_size\n",
    "\n",
    "train_indices = train_val_indices[:train_size]\n",
    "val_indices = train_val_indices[train_size:]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomRotation([-30, 30])\n",
    "    ])\n",
    "\n",
    "# apply data augmentation to the train data\n",
    "train_dataset.dataset.transform = transform\n",
    "val_dataset.dataset.transform = transform        \n",
    "test_dataset.dataset.transform = transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"PNC\" # DIRECTLY INPUT MODEL NAME HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if model_name == \"PNC\":\n",
    "    model = PNC_Autoencoder().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    max_tail_length = 10\n",
    "    train_autoencoder(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, model_name, max_tail_length)\n",
    "    \n",
    "if model_name == \"LRAE_VC\":\n",
    "    model = LRAE_VC_Autoencoder().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_autoencoder(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, model_name)\n",
    "\n",
    "    # Call the respective training function for autoencoder\n",
    "if model_name == \"PNC_with_classification\":\n",
    "    model = PNC_Autoencoder_with_Classification().to(device)\n",
    "    criterion_reconstruction = nn.MSELoss()\n",
    "    criterion_classification = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_combined_model(model, train_loader, val_loader, test_loader, criterion_reconstruction, criterion_classification, optimizer, device, num_epochs, model_name)\n",
    "\n",
    "\n",
    "\n",
    "# Save images generated by decoder \n",
    "output_path = \"output_test_imgs_post_training/\"\n",
    "if not os.path.exists(output_path):\n",
    "    print(f\"Creating directory: {output_path}\")\n",
    "    os.makedirs(output_path)\n",
    "else:\n",
    "    print(f\"Directory already exists: {output_path}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, _, filenames) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        if model_name == \"PNC_with_classification\": outputs, _ = model(inputs)\n",
    "        else: outputs = model(inputs)\n",
    "        for j in range(inputs.size()[0]):\n",
    "            output = outputs[j].permute(1, 2, 0).cpu().numpy() # outputs[j] original shape is (3, 224, 224), which need to convert to -> (224, 224, 3)\n",
    "            # output = (output * 255).astype(np.uint8)\n",
    "\n",
    "            # save the numpy array as image\n",
    "            plt.imsave(os.path.join(output_path, filenames[j]), output)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
